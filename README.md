# NLP_AttentionModels

Context: I completed these guided programming exercises during the Coursera course entitled 'Natural Language Processing with Attention Models'.

1. C4_W1_Assignment.ipynb: This notebook records the steps taken to build an English-to-German neural machine translation model. The decoder (LSTMs) uses a scaled dot product attention mechanism to access the entire encoded input sentence returned by the encoder (LSTM). Tokenisation is based on subword representations. Training is assisted by bucketing. Decoding is based on sampling or Minimum Bayes Risk decoding.

2. C4_W2_Assignment.ipynb: 
